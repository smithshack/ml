#softmax

import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score


# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a bias term (column of ones) to the data
X = np.c_[np.ones(X.shape[0]), X]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)

def compute_loss_and_gradients(X, y, theta):
    logits = X.dot(theta)
    y_proba = softmax(logits)
    m = X.shape[0]
    entropy_loss = -np.mean(np.log(y_proba[np.arange(m), y]))
    gradients = (1/m) * X.T.dot(y_proba - np.eye(np.max(y) + 1)[y])
    return entropy_loss, gradients

def predict(X, theta):
    logits = X.dot(theta)
    return np.argmax(softmax(logits), axis=1)





# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a bias term (column of ones) to the data
X = np.c_[np.ones(X.shape[0]), X]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a bias term (column of ones) to the data
X = np.c_[np.ones(X.shape[0]), X]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)






def softmax_regression(X_train, y_train, X_val, y_val, learning_rate=0.01, n_epochs=1000, tol=1e-4, patience=5):
    n_inputs = X_train.shape[1]
    n_outputs = np.max(y_train) + 1
    theta = np.random.randn(n_inputs, n_outputs)
    
    best_loss = np.inf
    epochs_without_improvement = 0
    
    for epoch in range(n_epochs):
        loss, gradients = compute_loss_and_gradients(X_train, y_train, theta)
        theta = theta - learning_rate * gradients
        
        val_loss, _ = compute_loss_and_gradients(X_val, y_val, theta)
        
        if val_loss < best_loss - tol:
            best_loss = val_loss
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1
            
        if epochs_without_improvement >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
            
    return theta

# Split the training data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Train the model
theta = softmax_regression(X_train_split, y_train_split, X_val_split, y_val_split)






# Predict and evaluate on the test set
y_pred = predict(X_test, theta)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy: {accuracy * 100:.2f}%")

